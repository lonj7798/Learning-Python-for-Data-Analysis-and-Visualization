{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Prediction Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression predictions should be unbiased. That is:\n",
    "\n",
    "<code>\"average of predictions\" should ≈ \"average of observations\"</code>\n",
    "\n",
    "**Prediction bias** is a quantity that measures how far apart those two averages are. That is:\n",
    "\n",
    "$$ prediction bias = average of predictions - average of labels in dataset $$\n",
    "\n",
    "###### Note: \"predicion bias\" is a different quantity than bias( the b in wx + b)\n",
    "\n",
    "A significant nonzero prediction bias tells you there is a bug somewhere in your model, as it indicates that the model is wrong about how frequently positive labels occur.\n",
    "\n",
    "For example, let's say we know that on average, 1% of all emails are spam. If we don't know anything at all about a given email, we should predict that it's 1% likely to be spam. Similarly, a good spam model should predict on average that emails are 1% likely to be spam. (In other words, if we average the predicted likelihoods of each individual email being spam, the result should be 1%.) If instead, the model's average prediction is 20% likelihood of being spam, we can conclude that it exhibits prediction bias.\n",
    "\n",
    "Possible root causes of prediction bias are:\n",
    "\n",
    "- Incomplete feature set\n",
    "- Noisy data set\n",
    "- Buggy pipeline\n",
    "- Biased training sample\n",
    "- Overly strong regularization\n",
    "\n",
    "You might be tempted to correct prediction bias by post-processing the learned model—that is, by adding a **calibration layer** that adjusts your model's output to reduce the prediction bias. For example, if your model has +3% bias, you could add a calibration layer that lowers the mean prediction by 3%. However, adding a calibration layer is a bad idea for the following reasons:\n",
    "\n",
    "- You're fixing the symptom rather than the cause.\n",
    "- You've built a more brittle system that you must now keep up to date.\n",
    "\n",
    "If possible, avoid calibration layers. Projects that use calibration layers tend to become reliant on them—using calibration layers to fix all their model's sins. Ultimately, maintaining the calibration layers can become a nightmare.\n",
    "\n",
    "###### Note: A good model will usually have near-zero bias. That said, a low prediction bias does not prove that your model is good. A really terrible model could have a zero prediction bias. For example, a model that just predicts the mean value for all examples would be a bad model, despite having zero bias.\n",
    "\n",
    "### Bucketing and Prediction Bias\n",
    "Logistic regression predicts a value between 0 and 1. However, all labeled examples are either exactly 0 (meaning, for example, \"not spam\") or exactly 1 (meaning, for example, \"spam\"). Therefore, when examining prediction bias, you cannot accurately determine the prediction bias based on only one example; you must examine the prediction bias on a \"bucket\" of examples. That is, prediction bias for logistic regression only makes sense when grouping enough examples together to be able to compare a predicted value (for example, 0.392) to observed values (for example, 0.394).\n",
    "\n",
    "You can form buckets in the following ways:\n",
    "\n",
    "- Linearly breaking up the target predictions.\n",
    "- Forming quantiles.\n",
    "\n",
    "Consider the following calibration plot from a particular model. Each dot represents a bucket of 1,000 values. The axes have the following meanings:\n",
    "\n",
    "- The x-axis represents the average of values the model predicted for that bucket.\n",
    "- The y-axis represents the actual average of values in the data set for that bucket.\n",
    "\n",
    "Both axes are logarithmic scales.\n",
    "\n",
    "![](img/13-15.png)\n",
    "\n",
    "Why are the predictions so poor for only part of the model? Here are a few possibilities:\n",
    "\n",
    "- The training set doesn't adequately represent certain subsets of the data space.\n",
    "- Some subsets of the data set are noisier than others.\n",
    "- The model is overly [regularized](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture). (Consider reducing the value of [lambda](https://developers.google.com/machine-learning/glossary#lambda).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
