{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Neural Networks\n",
    "\n",
    "Earlier, you encountered binary classification models that could pick between one of two possible choices, such as whether:\n",
    "\n",
    "- A given email is spam or not spam.\n",
    "- A given tumor is malignant or benign.\n",
    "\n",
    "In this module, we'll investigate **multi-class** classification, which can pick from multiple possibilities. For example:\n",
    "\n",
    "- Is this dog a beagle, a basset hound, or a bloodhound?\n",
    "- Is this flower a Siberian Iris, Dutch Iris, Blue Flag Iris, or Dwarf Bearded Iris?\n",
    "- Is that plane a Boeing 747, Airbus 320, Boeing 777, or Embraer 190?\n",
    "- Is this an image of an apple, bear, candy, dog, or egg?\n",
    "\n",
    "Some real-world multi-class problems entail choosing from millions of separate classes. For example, consider a multi-class classification model that can identify the image of just about anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Neural Networks: One vs. All\n",
    "\n",
    "**One vs. all** provides a way to **leverage binary classification**. Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate binary classifiers—one binary classifier for each possible outcome. During training, the model runs through a sequence of binary classifiers, training each to answer a separate classification question. For example, given a picture of a dog, five different recognizers might be trained, four seeing the image as a negative example (not a dog) and one seeing the image as a positive example (a dog). That is:\n",
    "\n",
    "1. Is this image an apple? No.\n",
    "2. Is this image a bear? No.\n",
    "3. Is this image candy? No.\n",
    "4. Is this image a dog? Yes.\n",
    "5. Is this image an egg? No.\n",
    "\n",
    "This approach is fairly reasonable when the total number of classes is small, but becomes increasingly inefficient as the number of classes rises.\n",
    "\n",
    "We can create a significantly more efficient one-vs.-all mocel with a deep neural network in which each output node represents a defferent class. The following figure suggests this approach:\n",
    "\n",
    "![](img/17-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Neural Networks: Softmax\n",
    "\n",
    "Recall that [logistic regression](https://developers.google.com/machine-learning/crash-course/logistic-regression/video-lecture?hl=en) produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.\n",
    "\n",
    "**Softmax** extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a **multi-class problem**. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
    "\n",
    "For example, returning to the image analysis we saw in Figure 1, Softmax might produce the following likelihoods of an image belonging to a particular class:\n",
    "\n",
    "![](img/17-2.png)\n",
    "\n",
    "Soft max is impemented through a nerual network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
    "\n",
    "![](img/17-3.png)\n",
    "\n",
    "\n",
    "\n",
    "This is Softmax equation is as follows:\n",
    "$$ p(y=j|x) = \\frac{e^{(W^T_jX+b_j)}}{\\sum_{k \\in K}{e^{(W^T_kX+b_k)}}} $$\n",
    "\n",
    "Note that this formula basically extends the formula for logistic regression into multiple classes.\n",
    "\n",
    "\n",
    "### Softmax Options\n",
    "\n",
    "Consider the following variants of Softmax:\n",
    "\n",
    "- **Full Softmax** is the Softmax we've been discussing; that is, Softmax calcuates a probability for every possible class.\n",
    "- **Candidate sampling** means that Softmax calcuates a probability for all the positive labels but only for a drandom sample of negative labels. For example, if we are interested in determining whether an input image is a beagle or a bloodhound, we don't have to provide probabilites for every non-doggy example.\n",
    "\n",
    "Full Softmax is fairly cheap when the number of classes is small but becomes prohibitively expensive when the number of classes climbs. Candidate sampling can improve efficiency in problems having a large number of classes.\n",
    "\n",
    "### One Label cs. Many Labels\n",
    "\n",
    "Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multople classes. For such examples:\n",
    "\n",
    "- You may not use Softmax\n",
    "- You must rely on mutiple logistic regressions.\n",
    "\n",
    "For example, suppose your exaples are images containing exactly one item-a piece of fruit. Softmax can determine the likelihood of that one item being a pear, an orange, an apple, and so on. If your examples are images containing all sorts of things—bowls of different kinds of fruit—then you'll have to use multiple logistic regressions instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
