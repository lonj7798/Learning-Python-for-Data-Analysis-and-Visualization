{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Reduce Loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [Reducing Loss: An Iterative Approach](https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach)\n",
    "\n",
    "Iterative strategies are prevalent in machine learning, primarily because they scale so well to large data sets.\n",
    "\n",
    "$$ y' = b + w_1x_1 $$\n",
    "\n",
    "Ex:\n",
    "You'll start with a wild guess (\"The value of $w_1$ is 0.\") and wait for the system to tell you what the loss is. Then, you'll try another guess (\"The value of $w_1$ is 0.5.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [Reducing Loss: Gradient Descent](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)\n",
    "\n",
    "(usually for square error which makes a quadric graph)\n",
    "![title](img/04-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [Reducing Loss: Learning Rate](https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate)\n",
    "\n",
    "Gradient descent algorithms multiply the gradient by a scalar known as the **learning rate** (also sometimes called **step size**) to determine the next point.\n",
    "\n",
    "**Hyperparameters** are the knobs that programmers tweak in machine learning algorithms. Most machine learning programmers spend a fair amount of time tuning the learning rate. If you pick a learning rate that is too small, learning will take too long:\n",
    "\n",
    "![title](img/04-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [Reducing Loss: Optimizing Learning Rate](https://developers.google.com/machine-learning/crash-course/fitter/graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [Reducing Loss: Stochastic Gradient Descent](https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent)\n",
    "\n",
    "In gradient descent, a **batch** is the total number of examples you use to calculate the gradient in a single iteration. So far, we've assumed that the batch has been the entire data set. If data set is contain huge numbers of features, a batch can be enormous. A very large batch may cause even a single iteration to take a very long time to compute.\n",
    "\n",
    "Some redundancy can be useful to smooth out noisy gradients, but enormous batches tend not to carry much more predictive value than large batches.\n",
    "\n",
    "What if we could get the right gradient on average for much less computation? By choosing examples at random from our data set, we could estimate (albeit, noisily) a big average from a much smaller one. **Stochastic gradient descent (SGD)** takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "\n",
    "**Mini-batch Stochastic Gradient Descent (mini-batch SGD)** is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.\n",
    "\n",
    "**stochastic gradient descent (SGD)**: A gradient descent algorithm in which the batch size is one. In other words, SGD relies on a single example chosen uniformly at random from a dataset to calculate an estimate of the gradient at each step.\n",
    "\n",
    "wiki: https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
