{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "This module shows how logistic regression can be used for classification tasks, and explores how to evaluate the effectiveness of classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Thresholding\n",
    "\n",
    "Logistic regression returns a probability. You can use the returned probability \"as is\" (for example, the probability that the user will click on this ad is 0.00023) or convert the returned probability to a binary value (for example, this email is spam).\n",
    "\n",
    "A logistic regression model that returns 0.9995 for a particular email message is predicting that it is very likely to be spam. Conversely, another email message with a prediction score of 0.0003 on that same logistic regression model is very likely not spam. However, what about an email message with a prediction score of 0.6? In order to map a logistic regression value to a binary category, you must define a **classification threshold** (also called the **decision threshold**). A value above that threshold indicates \"spam\"; a value below indicates \"not spam.\" It is tempting to assume that the classification threshold should always be 0.5, but thresholds are problem-dependent, and are therefore values that you must tune.\n",
    "\n",
    "The following sections take a closer look at metrics you can use to evaluate a classification model's predictions, as well as the impact of changing the classification threshold on these predictions.\n",
    "\n",
    "##### Note: \"Tuning\" a threshold for logistic regression is different from tuning hyperparameters such as learning rate. Part of choosing a threshold is assessing how much you'll suffer for making a mistake. For example, mistakenly labeling a non-spam message as spam is very bad. However, mistakenly labeling a spam message as non-spam is unpleasant, but hardly the end of your job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: True vs. False and Positive vs. Negative\n",
    "\n",
    "In this section, we'll define the primary building blocks of the metrics we'll use to evaluate classification models. But first, a fable:\n",
    "\n",
    "![](img/13-1.png)\n",
    "\n",
    "Let's make the following definitions:\n",
    "\n",
    "- \"Wolf\" is a **positive class**.\n",
    "- \"No wolf\" is a **negative class**.\n",
    "\n",
    "We can summarize our \"wolf-prediction\" model using a 2x2 [confusion matrix](https://developers.google.com/machine-learning/glossary#confusion_matrix) that depicts all four possible outcomes:\n",
    "\n",
    "![](img/13-2.png)\n",
    "\n",
    "A **true positive** is an outcome where the model correctly predicts the positive class. Similarly, a **true negative** is an outcome where the model correctly predicts the negative class.\n",
    "\n",
    "A **false positiv** is an outcome where the model incorrectly predicts the positive class. And a **false negative** is an outcome where the model incorrectly predicts the negative class.\n",
    "\n",
    "In the following sections, we'll look at how to evaluate classification models using metrics derived from these four outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition:\n",
    "\n",
    "$$ Accuracy = \\frac{Number \\ of \\ correct predictions}{Total \\ number \\ of \\ predictions} $$\n",
    "\n",
    "For binary classification, accuracy can also be calcuated in terms of positives and negatives as follows:\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Where *TP = True Positives, TN = True Negatives, FP = False Positives*, and *FN = False Negatives*.\n",
    "\n",
    "Let's try calculating accuracy for the following model that classified 100 tumors as [malignant](https://en.wikipedia.org/wiki/Malignancy) (the positive class) or [benign](https://en.wikipedia.org/wiki/Benign_tumor) (the negative class):\n",
    "\n",
    "![](img/13-3.png)\n",
    "\n",
    "Accuracy comes out to 0.91, or 91% (91 correct predictions out of 100 total examples). That means our tumor classifier is doing a great job of identifying malignancies, right?\n",
    "\n",
    "Actually, let's do a closer analysis of positives and negatives to gain more insight into our model's performance.\n",
    "\n",
    "Of the 100 tumor examples, 91 are benign (90 TNs and 1 FP) and 9 are malignant (1 TP and 8 FNs).\n",
    "\n",
    "Of the 91 benign tumors, the model correctly identifies 90 as benign. That's good. However, of the 9 malignant tumors, the model only correctly identifies 1 as malignant—a terrible outcome, as 8 out of 9 malignancies go undiagnosed!\n",
    "\n",
    "While 91% accuracy may seem good at first glance, another tumor-classifier model that always predicts benign would achieve the exact same accuracy (91/100 correct predictions) on our examples. In other words, our model is no better than one that has zero predictive ability to distinguish malignant tumors from benign tumors.\n",
    "\n",
    "Accuracy alone doesn't tell the full story when you're working with a **class-imbalanced data set**, like this one, where there is a significant disparity between the number of positive and negative labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Precision and Recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "**Precision** attempts to answer the followin question : \\\n",
    "<code>What proportion of positive identifications was actually correct?</code>\n",
    "\n",
    "Precision is defined as follows:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "###### Note: A model that produces no false positives has a precision of 1.0.\n",
    "\n",
    "Let's calculate precision for our ML model from the previous section that analyzes tumors:\n",
    "\n",
    "![](img/13-4.png)\n",
    "\n",
    "Our model has a precision of 0.5—in other words, when it predicts a tumor is malignant, it is correct 50% of the time.\n",
    "\n",
    "\n",
    "#### Recall\n",
    "\n",
    "**Recall** attempts to answer the following gquestion:\n",
    "\n",
    "<code>What proportion of acutal positives was identified correctly?</code>\n",
    "\n",
    "Mathematically, recall is defined as follows:\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Let's caculate recall for our tumor classifier:\n",
    "\n",
    "![](img/13-5.png)\n",
    "\n",
    "Our model has a recall of 0.11-in other words, it correctly identifies 11% of all maligant tumors.\n",
    "\n",
    "#### Precision and Recall: A Tug of War\n",
    "To fully evaluate the effectiveness of a model, you must examine **both** precision and recall. Unfortunately, precision and recall are often in tension. That is, improving precision typically reduces recall and vice versa. Explore this notion by looking at the following figure, which shows 30 predictions made by an email classification model. Those to the right of the classification threshold are classified as \"spam\", while those to the left are classified as \"not spam.\"\n",
    "\n",
    "![](img/13-6.png)\n",
    "\n",
    "Let's calculate precision and recall based on the results shown in Figure 1:\n",
    "\n",
    "![](img/13-7.png)\n",
    "\n",
    "Precision measures the percentage of **emails flagged as spam** that were correctly classified-that is, the percentage of dots to the right of the hreshold line that are green in Figure 1:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} = \\frac{8}{8 + 2} = 0.8 $$\n",
    "\n",
    "Recall measures the precentage of **actual spam emails** that were correctly classified-that is, the percentage of green dots that are to the right of the threshold line in Figure 1:\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} = \\frac{8}{8 + 3} = 0.73 $$\n",
    "\n",
    "Figure 2 illustrates the effect of increasing the classifiction threshold.\n",
    "\n",
    "![](img/13-8.png)\n",
    "\n",
    "The number of false positives decreases, but false negatives increase. As a result, precision increases, while recall decreases:\n",
    "\n",
    "![](img/13-9.png)\n",
    "\n",
    "Conversely, Figure 3 illustrates the effect of decreasing the classification threshold (from its original position in Figure 1).\n",
    "\n",
    "![](img/13-10.png)\n",
    "\n",
    "False positives increase, and false negatives decrease. As a result, this time, precision decreases and recall increases:\n",
    "\n",
    "![](img/13-11.png)\n",
    "\n",
    "Various metircs have been developed that rely on both precision and recall. For example, see [F1 score](https://en.wikipedia.org/wiki/F1_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
