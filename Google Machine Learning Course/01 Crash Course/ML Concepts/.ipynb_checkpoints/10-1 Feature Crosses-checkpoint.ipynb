{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Crosses\n",
    "\n",
    "A **feature cross** is a **synthetic feature** formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Crosses: Encoding Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Figures 1 and 2, imagine the following:\n",
    "\n",
    "- The blue dots represent sick trees.\n",
    "- The orange dots represent healthy trees.\n",
    "\n",
    "![](img/10-1.png)\n",
    "\n",
    "Can you draw a line that neatly separates the sick trees from the healthy trees? Sure. This is a linear problem. The line won't be perfect. A sick tree or two might be on the \"healthy\" side, but your line will be a good predictor.\n",
    "\n",
    "Now look at the following figure:\n",
    "\n",
    "![](img/10-2.png)\n",
    "\n",
    "Can you draw a single straight line that neatly separates the sick trees from the healthy trees? No, you can't. This is a nonlinear problem. Any line you draw will be a poor predictor of tree health.\n",
    "\n",
    "![](img/10-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the nonlinear problem shown in Figure 2, create a feature cross. A feature cross is a synthetic feature that encodes nonlinearity in the feature space by multiplying two or more input features together. (The term cross comes from cross product.) Let's create a feature cross named $x_3$ by crossing $x_1$ and $x_2$:\n",
    "\n",
    "$$x_3 = x_1x_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We treat this newly minted $x_3$ feature cross just like any other feature. The linear formula becomes:\n",
    "\n",
    "$$ y = b + w_1x_1 + w_2x_2 + w_3x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear algorithm can learn a weight for $w_3$ just as it would for $w_1$ and $w_2$. In other words, although $w_3$ encodes nonlinear information, you donâ€™t need to change how the linear model trains to determine the value of $w_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinds of feature crosses\n",
    "We can create many different kinds of feature crosses. For example:\n",
    "\n",
    "- $[A \\times B]$: a feature cross formed by multiplying the values of two features.\n",
    "- $[A \\times B \\times C \\times D \\times E]$: a feature cross formed by multiplying the values of five features.\n",
    "- $[A \\times A]$: a feature cross formed by squaring a single feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to [stochastic gradient descent](https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent), linear models can be trained efficiently. Consequently, supplementing scaled linear models with feature crosses has traditionally been an efficient way to train on massive-scale data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Crosses: Crossing One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've focused on feature-crossing two individual floating-point features. In practice, machine learning models seldom cross continuous features. However, machine learning models do frequently cross one-hot feature vectors. Think of feature crosses of one-hot feature vectors as logical conjunctions. For example, suppose we have two features: country and language. A one-hot encoding of each generates vectors with binary features that can be interpreted as <code>**country=USA**</code>, <code>**country=France**</code> or <code>**language=English**</code>, <code>**language=Spanish**</code>. Then, if you do a feature cross of these one-hot encodings, you get binary features that can be interpreted as logical conjunctions, such as:\n",
    "<pre><code>country:usa AND language:spanish</code></pre>\n",
    "\n",
    "As another example, suppose you bin latitude and longitude, producing separate one-hot five-element feature vectors. For instance, a given latitude and longitude could be represented as follows:\n",
    "<pre><code>\n",
    "binned_latitude = [0, 0, 0, 1, 0]\n",
    "binned_longitude = [0, 1, 0, 0, 0]\n",
    "</code></pre>\n",
    "\n",
    "Suppose you create a feature cross of these two feature vectors:\n",
    "<pre><code>binned_latitude X binned_longitude</code></pre>\n",
    "\n",
    "This feature cross is a 25-element one-hot vector (24 zeroes and 1 one). The single 1 in the cross identifies a particular conjunction of latitude and longitude. Your model can then learn particular associations about that conjunction.\n",
    "\n",
    "Suppose we bin latitude and longitude much more coarsely, as follows:\n",
    "<pre><code>\n",
    "binned_latitude(lat) = [\n",
    "  0  < lat <= 10\n",
    "  10 < lat <= 20\n",
    "  20 < lat <= 30\n",
    "]\n",
    "\n",
    "binned_longitude(lon) = [\n",
    "  0  < lon <= 15\n",
    "  15 < lon <= 30\n",
    "]\n",
    "</code></pre>\n",
    "\n",
    "Creating a feature cross of those coarse bins leads to synthetic feature having the following meanings:\n",
    "<pre><code>\n",
    "binned_latitude_X_longitude(lat, lon) = [\n",
    "  0  < lat <= 10 AND 0  < lon <= 15\n",
    "  0  < lat <= 10 AND 15 < lon <= 30\n",
    "  10 < lat <= 20 AND 0  < lon <= 15\n",
    "  10 < lat <= 20 AND 15 < lon <= 30\n",
    "  20 < lat <= 30 AND 0  < lon <= 15\n",
    "  20 < lat <= 30 AND 15 < lon <= 30\n",
    "]\n",
    "</code></pre>\n",
    "\n",
    "Now suppose our model needs to predict how satisfied dog owners will be with dogs based on two features:\n",
    "\n",
    "- Behavior type (barking, crying, snuggling, etc.)\n",
    "- Time of day\n",
    "If we build a feature cross from both these features:\n",
    "<pre><code>\n",
    "[behavior type X time of day]\n",
    "</code></pre>\n",
    "\n",
    "then we'll end up with vastly more predictive ability than either feature on its own. For example, if a dog cries (happily) at 5:00 pm when the owner returns from work will likely be a great positive predictor of owner satisfaction. Crying (miserably, perhaps) at 3:00 am when the owner was sleeping soundly will likely be a strong negative predictor of owner satisfaction.\n",
    "\n",
    "Linear learners scale well to massive data. Using feature crosses on massive data sets is one efficient strategy for learning highly complex models. [Neural networks](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks) provide another strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
