{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks: Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from the Feature Crosses unit, the following classification problem is nonlinear:\n",
    "\n",
    "![](img/15-1.png)\n",
    "\n",
    "\"Nonlinear\" means that you can't accurately predict a label with a model of the form $b + w_1x_1 + w_2x_2$ In other words, the \"decision surface\" is not a line. Previously, we looked at feature crosses as one possible approach to modeling nonlinear problems.\n",
    "\n",
    "Now consider the following data set:\n",
    "\n",
    "![](img/15-2.png)\n",
    "\n",
    "The data set shown in Figure 2 can't be solved with a linear model.\n",
    "\n",
    "To see how neural networks might help with nonlinear problems, let's start by representing a linear model as a graph:\n",
    "\n",
    "![](img/15-3.png)\n",
    "\n",
    "Each blue circle represents an input feature, and the green circle represents the weighted sum of the inputs.\n",
    "\n",
    "How can we alter this model to improve its ability to deal with nonlinear problems?\n",
    "\n",
    "\n",
    "#### Hidden Layers\n",
    "In the model represented by the following graph, we've added a \"hidden layer\" of intermediary values. Each yellow node in the hidden layer is a weighted sum of the blue input node values. The output is a weighted sum of the yellow nodes.\n",
    "\n",
    "![](img/15-4.png)\n",
    "\n",
    "Is this model linear? Yesâ€”its output is still a linear combination of its inputs.\n",
    "\n",
    "In the model represented by the following graph, we've added a second hidden layer of weighted sums.\n",
    "\n",
    "![](img/15-5.png)\n",
    "\n",
    "Figure 5. Graph of three-layer model.\n",
    "\n",
    "Is this model still linear? Yes, it is. When you express the output as a function of the input and simplify, you get just another weighted sum of the inputs. This sum won't effectively model the nonlinear problem in Figure 2.\n",
    "\n",
    "\n",
    "#### Activiation Functions\n",
    "\n",
    "To model a nonlinear problem, we can directly introduce a nonlinearity. We can pipe each hidden layer node through a nonlinear function.\n",
    "\n",
    "In the model represented by the following graph, the value of each node in Hidden Layer 1 is transformed by a nonlinear function before being passed on to the weighted sums of the next layer. This nonlinear function is called the activation function.\n",
    "\n",
    "![](img/15-6.png)\n",
    "\n",
    "Now that we've added an activation function, adding layers has more impact. Stacking nonlinearities on nonlinearities lets us model very complicated relationships between the inputs and the predicted outputs. In brief, each layer is effectively learning a more complex, higher-level function over the raw inputs. If you'd like to develop more intuition on how this works, see [Chris Olah's excellent blog post](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/).\n",
    "\n",
    "\n",
    "\n",
    "##### Common Activation Functions\n",
    "The following sigmoid activation function converts the weighted sum to a value between 0 and 1.\n",
    "\n",
    "$$F(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Here's a plot:\n",
    "\n",
    "![](img/15-7.png)\n",
    "\n",
    "The following **rectified linear unit** activation function (or **ReLU**, for short) often works a little better than a smooth function like the sigmoid, while also being significantly easier to compute.\n",
    "\n",
    "$$F(x) = \\max(0,x)$$\n",
    "\n",
    "The superiority of ReLU is based on empirical findings, probably driven by ReLU having a more useful range of responsiveness. A sigmoid's responsiveness falls off relatively quickly on both sides.\n",
    "\n",
    "![](img/15-8.png)\n",
    "\n",
    "In fact, any mathematical function can serve as an activation function. Suppose that  represents our activation function (Relu, Sigmoid, or whatever). Consequently, the value of a node in the network is given by the following formula:\n",
    "\n",
    "$$\\sigma(w \\cdot x+b)$$\n",
    "\n",
    "TensorFlow provides out-of-the-box support for many activation functions. You can find these activation functions within TensorFlow's [list of wrappers for primitive neural network operations](https://www.tensorflow.org/api_docs/python/tf/nn). That said, we still recommend starting with ReLU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
